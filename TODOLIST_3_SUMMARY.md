# âœ… TodoList 3ì¼ì°¨ êµ¬í˜„ ì™„ë£Œ ìš”ì•½

## ğŸ“Š êµ¬í˜„ í˜„í™©

| í•­ëª© | ìƒíƒœ | ì„¤ëª… |
|------|------|------|
| í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ëŸ¬ (Node-cron) | âœ… | ë§¤ì¼ ìì • ìë™ í¬ë¡¤ë§ |
| ë‹¹ê·¼ë§ˆì¼“ í¬ë¡¤ëŸ¬ | âœ… | ì´ë¯¸ êµ¬í˜„ë¨ |
| Redis ë°ì´í„° êµì²´ ë¡œì§ | âœ… | ë°±ì—… â†’ ìƒˆ ë°ì´í„° â†’ êµì²´ |
| í¬ë¡¤ë§ ë¡œê·¸ ì €ì¥ (PostgreSQL) | âœ… | crawling_logs í…Œì´ë¸” í™œìš© |
| ì—ëŸ¬ ì²˜ë¦¬ ë° ì¬ì‹œë„ ë¡œì§ | âœ… | ìµœëŒ€ 3íšŒ ì¬ì‹œë„ |
| Rate Limiting | âœ… | 2ì´ˆ ê°„ê²© + ì¬ì‹œë„ ì‹œ 5ì´ˆ ëŒ€ê¸° |

---

## ğŸ†• ìƒˆë¡œ ì¶”ê°€ëœ íŒŒì¼

### 1ï¸âƒ£ `backend/src/domain/crawling/repository/crawling-log.repository.ts`
- PostgreSQL `crawling_logs` í…Œì´ë¸” ê´€ë¦¬
- í¬ë¡¤ë§ ì‹œì‘/ì™„ë£Œ/ì‹¤íŒ¨ ë¡œê·¸ ì €ì¥
- í¬ë¡¤ë§ í†µê³„ ì¡°íšŒ

### 2ï¸âƒ£ `backend/src/domain/crawling/scheduler/crawling.scheduler.ts`
- Node-cron ê¸°ë°˜ ìŠ¤ì¼€ì¤„ëŸ¬
- ë§¤ì¼ ìì • ìë™ í¬ë¡¤ë§
- 4ì‹œê°„ë§ˆë‹¤ ë˜ëŠ” 1ë¶„ë§ˆë‹¤ ì˜µì…˜
- Asia/Seoul íƒ€ì„ì¡´ ì„¤ì •

---

## âœï¸ ìˆ˜ì •ëœ íŒŒì¼ (ê¸°ì¡´ ì½”ë“œ ê±´ë“œë¦¬ì§€ ì•ŠìŒ!)

### 1ï¸âƒ£ `backend/src/domain/crawling/service/crawling.service.ts`
**ì¶”ê°€ëœ ë©”ì„œë“œ:**
- `crawlDaangnByAreaWithRetry()`: ì¬ì‹œë„ ë¡œì§ (ìµœëŒ€ 3íšŒ, 5ì´ˆ ê°„ê²©)
- `crawlDaangnWithLogging()`: í¬ë¡¤ë§ ë¡œê·¸ ì €ì¥
- `crawlDaangnWithBackup()`: Redis ë°ì´í„° ë°±ì—… + êµì²´
- `getCrawlingStats()`: í¬ë¡¤ë§ í†µê³„ ì¡°íšŒ
- `getRecentCrawlingLogs()`: ìµœê·¼ í¬ë¡¤ë§ ë¡œê·¸ ì¡°íšŒ

### 2ï¸âƒ£ `backend/src/domain/crawling/repository/crawling.repository.ts`
**ì¶”ê°€ëœ ë©”ì„œë“œ:**
- `backupAllData()`: Redis ë°ì´í„° ë°±ì—…
- `restoreFromBackup()`: ë°±ì—…ì—ì„œ ë³µì›
- `deleteBackupData()`: ë°±ì—… ë°ì´í„° ì‚­ì œ

### 3ï¸âƒ£ `backend/src/domain/crawling/utils/container.ts`
- `CrawlingLogRepository` ì¶”ê°€
- `CrawlingScheduler` ì¶”ê°€
- `CrawlingService`ì— `CrawlingLogRepository` ì£¼ì…

### 4ï¸âƒ£ `backend/src/domain/crawling/index.ts`
- ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™” ë¡œì§ ì¶”ê°€
- `ENABLE_CRAWLER_SCHEDULER` í™˜ê²½ë³€ìˆ˜ë¡œ ì œì–´

### 5ï¸âƒ£ `backend/package.json`
- `node-cron`: "^3.0.3" ì¶”ê°€
- `@types/node-cron`: "^3.0.11" ì¶”ê°€

### 6ï¸âƒ£ `QUICK_START.md`
- í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ëŸ¬ ì‚¬ìš©ë²• ì¶”ê°€
- í™˜ê²½ë³€ìˆ˜ ì„¤ì • ì•ˆë‚´

---

## ğŸš€ ì‚¬ìš© ë°©ë²•

### 1ï¸âƒ£ ìë™ í¬ë¡¤ë§ í™œì„±í™”

#### docker-compose.ymlì— í™˜ê²½ë³€ìˆ˜ ì¶”ê°€:
```yaml
services:
  backend:
    environment:
      ENABLE_CRAWLER_SCHEDULER: "true"
```

#### ì¬ë¹Œë“œ ë° ì‹¤í–‰:
```bash
cd /Users/deviantce/used\ trade

# Clean Build
docker-compose down -v
docker-compose build --no-cache
docker-compose up -d

# ë¡œê·¸ í™•ì¸
docker-compose logs -f backend | grep "ìŠ¤ì¼€ì¤„"
```

### 2ï¸âƒ£ ìˆ˜ë™ í¬ë¡¤ë§ (ê¸°ì¡´ ë°©ì‹)

```bash
# ê¸°ë³¸ í¬ë¡¤ë§
curl -X POST http://localhost:3000/api/crawling/trigger \
  -H "Content-Type: application/json" \
  -d '{}'
```

### 3ï¸âƒ£ í¬ë¡¤ë§ ë¡œê·¸ í™•ì¸

í¬ë¡¤ë§ ì‹¤í–‰ ì‹œ PostgreSQL `crawling_logs` í…Œì´ë¸”ì— ìë™ ì €ì¥ë©ë‹ˆë‹¤:

```sql
SELECT * FROM crawling_logs ORDER BY started_at DESC LIMIT 10;
```

ë˜ëŠ” Docker ë¡œê·¸:

```bash
docker-compose logs -f backend
```

---

## ğŸ¯ ì£¼ìš” ê¸°ëŠ¥

### 1ï¸âƒ£ í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ëŸ¬
```typescript
// ë§¤ì¼ ìì • (ê¸°ë³¸)
scheduler.scheduleDailyCrawling();

// 4ì‹œê°„ë§ˆë‹¤
scheduler.scheduleEvery4Hours();

// í…ŒìŠ¤íŠ¸ìš©: 1ë¶„ë§ˆë‹¤
scheduler.scheduleEveryMinute();

// ì‹œì‘
scheduler.startAll();
```

### 2ï¸âƒ£ Redis ë°ì´í„° ë°±ì—… & êµì²´
```typescript
const result = await crawlingService.crawlDaangnWithBackup();
// 1. ê¸°ì¡´ ë°ì´í„° ë°±ì—… (*:items â†’ *:items:backup)
// 2. ìƒˆ ë°ì´í„° í¬ë¡¤ë§
// 3. ì„±ê³µ ì‹œ ë°±ì—… ì‚­ì œ, ì‹¤íŒ¨ ì‹œ ë³µì›
```

### 3ï¸âƒ£ ì¬ì‹œë„ ë¡œì§
```typescript
// ìë™ ì¬ì‹œë„ (ìµœëŒ€ 3íšŒ, 5ì´ˆ ê°„ê²©)
private async crawlDaangnByAreaWithRetry(area: Area, retryCount: number = 0)
```

### 4ï¸âƒ£ í¬ë¡¤ë§ ë¡œê·¸ ì €ì¥
```typescript
// ì‹œì‘ ë¡œê·¸
const logId = await crawlingLogRepo.startLog('daangn');

// ì™„ë£Œ ë¡œê·¸
await crawlingLogRepo.completeLog(logId, totalProducts, ...);

// ì‹¤íŒ¨ ë¡œê·¸
await crawlingLogRepo.failLog(logId, errorMessage, duration);
```

---

## ğŸ“‹ ë°ì´í„°ë² ì´ìŠ¤

### crawling_logs í…Œì´ë¸” êµ¬ì¡°

| ì»¬ëŸ¼ | íƒ€ì… | ì„¤ëª… |
|------|------|------|
| id | BIGSERIAL | ë¡œê·¸ ID (PK) |
| platform | VARCHAR(50) | í”Œë«í¼ëª… (daangn) |
| status | VARCHAR(50) | ìƒíƒœ (running/completed/failed) |
| total_products | INTEGER | ì´ ìƒí’ˆ ìˆ˜ |
| new_products | INTEGER | ìƒˆ ìƒí’ˆ ìˆ˜ |
| updated_products | INTEGER | ì—…ë°ì´íŠ¸ ìƒí’ˆ ìˆ˜ |
| error_count | INTEGER | ì—ëŸ¬ íšŸìˆ˜ |
| duration | INTEGER | ì†Œìš” ì‹œê°„ (ì´ˆ) |
| error_message | TEXT | ì—ëŸ¬ ë©”ì‹œì§€ |
| started_at | TIMESTAMP | ì‹œì‘ ì‹œê°„ |
| completed_at | TIMESTAMP | ì™„ë£Œ ì‹œê°„ |

### ì¸ë±ìŠ¤
- `idx_crawling_logs_platform`
- `idx_crawling_logs_started_at`
- `idx_crawling_logs_status`

---

## ğŸ”§ í™˜ê²½ë³€ìˆ˜

### ìƒˆë¡œ ì¶”ê°€ëœ í™˜ê²½ë³€ìˆ˜:
```env
ENABLE_CRAWLER_SCHEDULER=true  # í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ëŸ¬ í™œì„±í™”
```

### ê¸°ì¡´ í™˜ê²½ë³€ìˆ˜:
```env
NODE_ENV=production
PORT=3000
DB_HOST=postgres
DB_PORT=5432
DB_NAME=smarttrade
DB_USER=postgres
DB_PASSWORD=postgres
REDIS_HOST=redis
REDIS_PORT=6379
```

---

## ğŸ› ë¬¸ì œ í•´ê²°

### ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì‘ë™í•˜ì§€ ì•Šì•„ìš”
```bash
# 1. í™˜ê²½ë³€ìˆ˜ í™•ì¸
docker-compose exec backend printenv | grep SCHEDULER

# 2. ë¡œê·¸ í™•ì¸
docker-compose logs backend | grep "ìŠ¤ì¼€ì¤„ëŸ¬"

# 3. ìˆ˜ë™ìœ¼ë¡œ í™˜ê²½ë³€ìˆ˜ ì¶”ê°€
docker-compose up -d --build --force-recreate
```

### í¬ë¡¤ë§ ë¡œê·¸ê°€ ì €ì¥ë˜ì§€ ì•Šì•„ìš”
```bash
# 1. crawling_logs í…Œì´ë¸” í™•ì¸
docker-compose exec postgres psql -U postgres -d smarttrade -c "\d crawling_logs"

# 2. ì—†ìœ¼ë©´ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰
docker-compose exec backend npx ts-node scripts/migrate.ts
```

### Redis ë°±ì—…ì´ ì‘ë™í•˜ì§€ ì•Šì•„ìš”
```bash
# 1. Redis í‚¤ í™•ì¸
docker-compose exec redis redis-cli KEYS "*"

# 2. ë°±ì—… í‚¤ í™•ì¸
docker-compose exec redis redis-cli KEYS "*:backup"
```

---

## ğŸ“Š ì„±ëŠ¥ & ìµœì í™”

### Rate Limiting
- ì§€ì—­ ê°„ í¬ë¡¤ë§: **2ì´ˆ ê°„ê²©**
- ì¬ì‹œë„ ì‹œ: **5ì´ˆ ê°„ê²©**
- ìµœëŒ€ ì¬ì‹œë„: **3íšŒ**
- Timeout: **30ì´ˆ**

### Redis TTL
- ìƒí’ˆ ë°ì´í„°: **24ì‹œê°„**
- ë°±ì—… ë°ì´í„°: **24ì‹œê°„**

### í¬ë¡¤ë§ ì‹œê°„ (ì˜ˆìƒ)
- 1ê°œ ì§€ì—­: ~3ì´ˆ
- 10ê°œ ì§€ì—­: ~30ì´ˆ
- 50ê°œ ì§€ì—­: ~2.5ë¶„

---

## ğŸ‰ ì™„ë£Œ!

**TodoList 3ì¼ì°¨ (í¬ë¡¤ë§ ì‹œìŠ¤í…œ) 100% ì™„ë£Œ!**

### êµ¬í˜„ëœ ê¸°ëŠ¥:
- âœ… í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ëŸ¬ (ë§¤ì¼ ìì •)
- âœ… í¬ë¡¤ë§ ë¡œê·¸ ì €ì¥ (PostgreSQL)
- âœ… Redis ë°ì´í„° ë°±ì—… & êµì²´
- âœ… ì—ëŸ¬ ì²˜ë¦¬ & ì¬ì‹œë„ ë¡œì§
- âœ… Rate Limiting

### ê¸°ì¡´ ì½”ë“œ:
- âœ… ì „í˜€ ê±´ë“œë¦¬ì§€ ì•ŠìŒ
- âœ… ê¸°ì¡´ ê¸°ëŠ¥ ëª¨ë‘ ì •ìƒ ì‘ë™
- âœ… í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€

---

## ğŸ”— ê´€ë ¨ íŒŒì¼

**Repository:**
- `backend/src/domain/crawling/repository/crawling-log.repository.ts`
- `backend/src/domain/crawling/repository/crawling.repository.ts`

**Service:**
- `backend/src/domain/crawling/service/crawling.service.ts`

**Scheduler:**
- `backend/src/domain/crawling/scheduler/crawling.scheduler.ts`

**Container:**
- `backend/src/domain/crawling/utils/container.ts`

**Database:**
- `backend/scripts/migrate.ts`

**Documentation:**
- `QUICK_START.md`
- `TODOLIST_3_SUMMARY.md` (ì´ íŒŒì¼)

